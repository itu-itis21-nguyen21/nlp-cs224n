{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWccTzStoQDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38dff88a-6b49-4421-aa0e-b8a6c8de7f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'gdrive/My Drive/CS224n2021/minbert-default-final-project'\n",
            "/content/gdrive/My Drive/CS224n2021/minbert-default-final-project\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/My\\ Drive/CS224n2021/minbert-default-final-project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn\n",
        "!pip install tokenizers\n",
        "!pip install explainaboard_client"
      ],
      "metadata": {
        "id": "O64z2u8Tp5iD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b31a056-9a9e-4e39-87b5-d126520781bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post7.tar.gz (3.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.13.3)\n",
            "Requirement already satisfied: explainaboard_client in /usr/local/lib/python3.10/dist-packages (0.1.4)\n",
            "Requirement already satisfied: explainaboard-api-client>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from explainaboard_client) (0.4.3)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from explainaboard-api-client>=0.4.3->explainaboard_client) (2.0.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from explainaboard-api-client>=0.4.3->explainaboard_client) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->explainaboard-api-client>=0.4.3->explainaboard_client) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Pretrain BERT on SST and cfimdb datasets"
      ],
      "metadata": {
        "id": "RcTc95C5oRO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 classifier.py --option pretrain --use_gpu --epochs 10 --lr 1e-3 --batch_size 8 --hidden_dropout_prob 0.3"
      ],
      "metadata": {
        "id": "ZbSkFM8kpSfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seed 11711 - SST dev acc: 0.389, cfimdb dev acc: 0.780\n"
      ],
      "metadata": {
        "id": "z8oHKREKoczI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Finetune on SST and cfimdb datasets"
      ],
      "metadata": {
        "id": "NNfCWMtj9Zca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 classifier.py --option finetune --use_gpu --epochs 10 --lr 1e-5 --batch_size 8 --hidden_dropout_prob 0.3"
      ],
      "metadata": {
        "id": "O5dZoF939gaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis layer: 1 fully connected layer with dropout (p=0.3)\n",
        "\n",
        "Seed 11711 - SST dev acc: 0.535, cfimdb dev acc: 0.963\n",
        "\n",
        "Sentiment analysis layer: 1 fully connected layer without dropout\n",
        "\n",
        "Seed 11711 - SST dev acc: 0.532, cfimdb dev acc: 0.971"
      ],
      "metadata": {
        "id": "y3gm-qwoKzq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Multi-task BERT**"
      ],
      "metadata": {
        "id": "cOliaz1bjI-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 multitask_classifier.py --option finetune --use_gpu --epochs 5 --lr 1e-5 --batch_size 8 --hidden_dropout_prob 0.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX0hV-AojGBu",
        "outputId": "749201c4-1575-4fb3-b22e-926a8ceb7056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v10\n",
            "v27\n",
            "Loaded 8544 train examples from data/ids-sst-train.csv\n",
            "Loaded 141498 train examples from data/quora-train.csv\n",
            "Loaded 6040 train examples from data/sts-train.csv\n",
            "Loaded 1101 train examples from data/ids-sst-dev.csv\n",
            "Loaded 20212 train examples from data/quora-dev.csv\n",
            "Loaded 863 train examples from data/sts-dev.csv\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 3.94MB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 3.02MB/s]\n",
            "Downloading: 100% 440M/440M [00:05<00:00, 85.0MB/s]\n",
            "train-0: 252it [01:13,  3.44it/s]\n",
            "eval: 100% 5896/5896 [03:28<00:00, 28.32it/s]\n",
            "eval: 100% 252/252 [00:08<00:00, 30.30it/s]\n",
            "eval: 100% 356/356 [00:08<00:00, 40.75it/s]\n",
            "Paraphrase detection accuracy: 0.629\n",
            "Sentiment classification accuracy: 0.315\n",
            "Semantic Textual Similarity correlation: 0.234\n",
            "eval: 100% 2527/2527 [01:28<00:00, 28.49it/s]\n",
            "eval: 100% 108/108 [00:03<00:00, 28.42it/s]\n",
            "eval: 100% 138/138 [00:03<00:00, 40.98it/s]\n",
            "Paraphrase detection accuracy: 0.625\n",
            "Sentiment classification accuracy: 0.278\n",
            "Semantic Textual Similarity correlation: 0.211\n",
            "save the model to finetune-5-1e-05-paraphrasing.pt\n",
            "Epoch 0: train loss :: 4.870, train mean acc:: 0.393, dev mean acc:: 0.371\n",
            "train-1: 252it [01:16,  3.29it/s]\n",
            "eval: 100% 5896/5896 [03:28<00:00, 28.27it/s]\n",
            "eval: 100% 252/252 [00:08<00:00, 28.24it/s]\n",
            "eval: 100% 356/356 [00:08<00:00, 41.24it/s]\n",
            "Paraphrase detection accuracy: 0.654\n",
            "Sentiment classification accuracy: 0.290\n",
            "Semantic Textual Similarity correlation: 0.315\n",
            "eval: 100% 2527/2527 [01:28<00:00, 28.43it/s]\n",
            "eval: 100% 108/108 [00:03<00:00, 30.08it/s]\n",
            "eval: 100% 138/138 [00:03<00:00, 44.45it/s]\n",
            "Paraphrase detection accuracy: 0.651\n",
            "Sentiment classification accuracy: 0.256\n",
            "Semantic Textual Similarity correlation: 0.246\n",
            "save the model to finetune-5-1e-05-paraphrasing.pt\n",
            "Epoch 1: train loss :: 4.416, train mean acc:: 0.420, dev mean acc:: 0.384\n",
            "train-2: 252it [01:16,  3.32it/s]\n",
            "eval: 100% 5896/5896 [03:28<00:00, 28.23it/s]\n",
            "eval: 100% 252/252 [00:09<00:00, 27.36it/s]\n",
            "eval: 100% 356/356 [00:08<00:00, 41.89it/s]\n",
            "Paraphrase detection accuracy: 0.665\n",
            "Sentiment classification accuracy: 0.284\n",
            "Semantic Textual Similarity correlation: 0.315\n",
            "eval: 100% 2527/2527 [01:28<00:00, 28.66it/s]\n",
            "eval: 100% 108/108 [00:04<00:00, 26.54it/s]\n",
            "eval: 100% 138/138 [00:03<00:00, 43.61it/s]\n",
            "Paraphrase detection accuracy: 0.655\n",
            "Sentiment classification accuracy: 0.262\n",
            "Semantic Textual Similarity correlation: 0.261\n",
            "save the model to finetune-5-1e-05-paraphrasing.pt\n",
            "Epoch 2: train loss :: 4.235, train mean acc:: 0.421, dev mean acc:: 0.392\n",
            "train-3: 252it [01:16,  3.31it/s]\n",
            "eval: 100% 5896/5896 [03:26<00:00, 28.52it/s]\n",
            "eval: 100% 252/252 [00:08<00:00, 29.41it/s]\n",
            "eval: 100% 356/356 [00:08<00:00, 41.35it/s]\n",
            "Paraphrase detection accuracy: 0.685\n",
            "Sentiment classification accuracy: 0.427\n",
            "Semantic Textual Similarity correlation: 0.500\n",
            "eval: 100% 2527/2527 [01:27<00:00, 28.84it/s]\n",
            "eval: 100% 108/108 [00:03<00:00, 30.27it/s]\n",
            "eval: 100% 138/138 [00:03<00:00, 40.01it/s]\n",
            "Paraphrase detection accuracy: 0.681\n",
            "Sentiment classification accuracy: 0.401\n",
            "Semantic Textual Similarity correlation: 0.319\n",
            "save the model to finetune-5-1e-05-paraphrasing.pt\n",
            "Epoch 3: train loss :: 4.027, train mean acc:: 0.537, dev mean acc:: 0.467\n",
            "train-4: 252it [01:16,  3.31it/s]\n",
            "eval: 100% 5896/5896 [03:27<00:00, 28.40it/s]\n",
            "eval: 100% 252/252 [00:08<00:00, 28.05it/s]\n",
            "eval: 100% 356/356 [00:08<00:00, 42.58it/s]\n",
            "Paraphrase detection accuracy: 0.693\n",
            "Sentiment classification accuracy: 0.460\n",
            "Semantic Textual Similarity correlation: 0.543\n",
            "eval: 100% 2527/2527 [01:29<00:00, 28.30it/s]\n",
            "eval: 100% 108/108 [00:03<00:00, 28.88it/s]\n",
            "eval: 100% 138/138 [00:03<00:00, 44.17it/s]\n",
            "Paraphrase detection accuracy: 0.692\n",
            "Sentiment classification accuracy: 0.431\n",
            "Semantic Textual Similarity correlation: 0.339\n",
            "save the model to finetune-5-1e-05-paraphrasing.pt\n",
            "Epoch 4: train loss :: 3.785, train mean acc:: 0.565, dev mean acc:: 0.487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Default model (trained only on STS dataset)**\n",
        "\n",
        "Paraphase detection acc: 0.425 (pooler_output), 0.378 (last_hidden_state then mean_pooling)\n",
        "\n",
        "Sentiment classification acc: 0.524 (pooler_output)\n",
        "\n",
        "STS correlation: 0.249 (pooler_output), 0.471 (last_hidden_state then mean_pooling)\n",
        "\n",
        "**Multi-task model in which the total loss is the sum of losses of 3 tasks (4 epochs)**\n",
        "\n",
        "Paraphase detection acc: 0.692\n",
        "\n",
        "Sentiment classification acc: 0.431\n",
        "\n",
        "STS correlation: 0.339\n"
      ],
      "metadata": {
        "id": "L3wdyhrIxdIQ"
      }
    }
  ]
}